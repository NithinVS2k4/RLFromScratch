





import numpy as np
import matplotlib.pyplot as plt
import sys
import scipy
import time
from IPython.display import display, clear_output

import gymnasium as gym

from collections import defaultdict, deque
import warnings

import torch
import torch.nn as nn
import torch.nn.functional as F





CONFIG = {
    'PPO': {
        'CartPole-v1': {
            'gamma': 0.99, 
            'total_steps': 10_000,
            'steps_per_batch': 128, 
            'policy_lr': 1e-3,
            'val_fn_lr': 5e-4, 
            'num_epochs': 10,
            'epsilon': 0.2,      
            'val_fn_hidden_sizes': [64,64],
        },
        'LunarLander-v3': {
            'gamma': 0.99, 
            'total_steps': 1_000_000,
            'steps_per_batch': 1024, 
            'policy_lr': 1e-3,
            'val_fn_lr': 5e-4, 
            'num_epochs': 10,
            'epsilon': 0.2,      
            'val_fn_hidden_sizes': [64,64],
        },
        'Acrobot-v1': {
            'gamma': 0.99,            
            'num_episodes': 500, 
            'policy_lr': 1e-3,
            'policy_epochs': 5,
            'epsilon': 0.3,
            'val_fn_lr': 5e-4,        
            'val_fn_epochs': 20,      
            'val_fn_hidden_sizes': [64,64], 
            'steps_per_batch': 2048,
            'num_minibatches': 16,
        },
        'Pendulum-v1': {
            'gamma': 0.99, 
            'total_steps': 5_000_000,
            'steps_per_batch': 1000, 
            'policy_lr': 3e-4,
            'val_fn_lr': 1e-3, 
            'num_epochs': 10,
            'epsilon': 0.2,      
            'val_fn_hidden_sizes': [64,64],
        },
        'BipedalWalker-v3': {
            'gamma': 0.99,            
            'num_episodes': 5000, 
            'policy_lr': 3e-4,
            'policy_epochs': 1,
            'epsilon': 0.2,
            'val_fn_lr': 1e-4,        
            'val_fn_epochs': 10,      
            'val_fn_hidden_sizes': [64,64],
            'steps_per_batch': 2048,
            'num_minibatches': 16,
        }
    },
    'MLPPolicy_hidden_sizes': [64,64],
    'animation': {
        'FPS': 45,
        'do_truncate': True,
    },
    'plot_stats_size': 20,
}





cartpole_env = gym.make('CartPole-v1', render_mode = 'rgb_array')
lunar_env = gym.make('LunarLander-v3', render_mode = 'rgb_array')
acrobot_env =  gym.make("Acrobot-v1", render_mode="rgb_array")

pendulum_env = gym.make('Pendulum-v1', render_mode = 'rgb_array')
bipedal_env = gym.make('BipedalWalker-v3', render_mode = 'rgb_array')





def animate_policy(env, policy, FPS: int = 12, do_truncate: bool = True):
    '''
    Animates a learned policy using the environment's render() method.
    
    Args:
        env: OpenAI Gym environment
        policy: A callable that takes in a state and returns a probability distribution over actions
        FPS (int): Frames per second of the animation
        do_truncate (bool): If True, truncate the animation at episode end or after 100 steps
    '''

    figure_size = (5, 5)
    
    env_id = env.unwrapped.spec.id

    if isinstance(env.action_space, gym.spaces.Discrete):
        is_discrete = True
        s_dims = env.observation_space.shape[0]
        a_dims = env.action_space.n
    elif isinstance(env.action_space, gym.spaces.Box):
        is_discrete = False
        s_dims = env.observation_space.shape[0]
        a_dims = env.action_space.shape[0]
    else:
        raise NotImplementedError(f"Unsupported action space {type(env.action_space)}")

    s, _ = env.reset()

    gray_sqr = "\U00002B1C"
    green_sqr = "\U0001F7E9"
    
    env_info = {
        'CartPole-v1': {
            'actions': lambda a: ["←","→"][a],
            'state_interpreter': lambda s: f"Position = {s[0]:.3f} m\nAngle = {s[2]*180/np.pi:.1f}°"
        },
        'LunarLander-v3': {
            'actions': lambda a: ["□□□", "■□□", "□■□", "□□■"][a],
            'state_interpreter': lambda s: (
                f"(x,y) = ({s[0]:.1f} m, {s[1]:.1f} m)\n"
                f"Leg Contact = ({bool(s[7])}, {bool(s[6])})"
            )
        },
        'MountainCar-v0': {
            'actions': lambda a: ["←","X","→"][a],
            'state_interpreter': lambda s: f"({s[0]:.2f}m, {s[1]:.2f}ms⁻¹)"
        },
        'Acrobot-v1': {
            'actions': lambda a: ["←","X","→"][a],
            'state_interpreter': lambda s: f"({np.arccos(s[0]):.2f}°, {np.arccos(s[1]):.2f}°)" 
        },
        'BipedalWalker-v3': {
            'actions': lambda a: np.round(np.asarray(a),2),
            'state_interpreter': lambda s: f"",
        }
    }

    step = 0
    
    while True:
        start_time = time.time()
        if is_discrete: 
            probs = policy(torch.as_tensor(s))
            dist =  torch.distributions.Categorical(probs)
            action = dist.sample().item()
        else: 
            mu, std = policy(torch.as_tensor(s))
            dist = torch.distributions.Normal(mu, std)
            action = dist.sample()
            
        step += 1

        clear_output(wait=True)
        frame = env.render()

        plt.figure(figsize=figure_size)
        plt.imshow(frame)
        plt.axis('off')

        if env_id in env_info:
            interp = env_info[env_id]['state_interpreter'](s)
            action_str = env_info[env_id]['actions'](action)
        else:
            interp = ""
            action_str = str(action)

        # Add information below the image
        plt.text(0.5, -0.15, f"State: {interp}\nAction: {action_str}\nTime Step: {step}", 
         transform=plt.gca().transAxes, fontsize=12, 
         verticalalignment='bottom', horizontalalignment='center')


        plt.show()

        s, r, terminated, truncated, _ = env.step(action)
        r = float(r)
        
        end_time = time.time()
        if FPS:
            time.sleep(max(0,1 / FPS - (end_time - start_time)))
            
        if terminated or (truncated and do_truncate):
            break

    # Show final frame
    clear_output(wait=True)
    frame = env.render()

    plt.figure(figsize=figure_size)
    plt.imshow(frame)
    plt.axis('off')

    if env_id in env_info:
        interp = env_info[env_id]['state_interpreter'](s)
    else:
        interp = ""

    plt.text(0.5, -0.15, f"Final State: {interp}\nAction: {action_str}\nTime Step: {step}", 
         transform=plt.gca().transAxes, fontsize=12, 
         verticalalignment='bottom', horizontalalignment='center')


    plt.show()



def plot_stats(stats, window_size = 25):
    def moving_average(x, window_size=25):
        return np.convolve(x, np.ones(window_size)/window_size, mode='valid')

    fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12,4))
    
    episode_lengths = stats['rewards']
    smoothed_lengths = moving_average(episode_lengths, window_size=window_size)
    
    ax1.plot(np.arange(1, len(smoothed_lengths) + 1),smoothed_lengths)
    ax1.set_title("Smoothed Rewards")
    ax1.set_xlabel("Episode")
    ax1.set_ylabel("Reward")

    val_loss = stats['ppo_loss']
    smoothed_loss = moving_average(val_loss, window_size=window_size)
    
    ax2.plot(np.arange(1,len(smoothed_loss) + 1), smoothed_loss)
    ax2.set_title("Smoothed Policy Function Loss")
    ax2.set_xlabel("Episode")
    ax2.set_ylabel("Loss")
    
    plt.show()
    
    print(f"{stats['completions']} Completions")





class MLPPolicyDiscrete(nn.Module):
    def __init__(self, state_dims, act_dims, hidden_sizes=[32]):
        super().__init__()
        layer_sizes = [state_dims] + hidden_sizes + [act_dims]
        layers = []
       
        for i in range(len(hidden_sizes)):
            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            layers.append(nn.ReLU()) 

        layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))
        
        self.mlp = nn.Sequential(*layers)

    def forward(self, state):
        logits = self.mlp(state)
        return F.softmax(logits, dim = -1).clamp(min=1e-8, max=1-1e-8)


class MLPPolicyContinuous(nn.Module):
    def __init__(self, state_dims, act_dims, hidden_sizes=[32]):
        super().__init__()
        layer_sizes = [state_dims] + hidden_sizes 
        layers = []
       
        for i in range(len(hidden_sizes)):
            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            layers.append(nn.ReLU()) 
        
        self.mlp = nn.Sequential(*layers)
        self.mu_head = nn.Linear(hidden_sizes[-1], act_dims)
        self.tanh = nn.Tanh()
        self.var_head = nn.Linear(hidden_sizes[-1], act_dims)
        self.softplus = nn.Softplus()

    def forward(self, state):
        x = self.mlp(state)
        mu = self.tanh(self.mu_head(x))
        var = self.softplus(self.var_head(x))
        return mu, var

class MLPPolicyContinuous2(nn.Module):
    def __init__(self, state_dims, act_dims, hidden_sizes=[32]):
        super().__init__()
        layer_sizes = [state_dims] + hidden_sizes 
        layers = []
       
        for i in range(len(hidden_sizes)):
            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            layers.append(nn.ReLU()) 

        layers.append(nn.Linear(hidden_sizes[-1] , act_dims))
        
        self.mlp = nn.Sequential(*layers)

    def forward(self, state):
        mu = self.mlp(state)
        return mu, torch.zeros_like(mu) + 0.2





class MLPValue(nn.Module):
    def __init__(self, state_dims, hidden_sizes=[16]):
        super().__init__()
        layer_sizes = [state_dims] + hidden_sizes + [1]
        layers = []
        
        for i in range(len(hidden_sizes)):
            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            layers.append(nn.ReLU()) 

        layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))
        
        self.mlp = nn.Sequential(*layers)

    def forward(self, state):
        return self.mlp(state).squeeze(-1)





def PPO2(env, policy, gamma = 1.0, num_episodes = 10,
         policy_lr = 1e-3, policy_epochs = 10, epsilon = 0.2,
         val_fn_lr = 1e-3, val_fn_epochs = 10, val_fn_hidden_sizes = [16],
         steps_per_batch = 2048, num_minibatches = 16):

    warnings.filterwarnings("ignore", category=DeprecationWarning)
    
    if isinstance(env.action_space, gym.spaces.Discrete):
        is_discrete = True
        s_dims = env.observation_space.shape[0]
        a_dims = env.action_space.n
    elif isinstance(env.action_space, gym.spaces.Box):
        is_discrete = False
        s_dims = env.observation_space.shape[0]
        a_dims = env.action_space.shape[0]
    else:
        raise NotImplementedError(f"Unsupported action space {type(env.action_space)}")
    
    stats = {'ep_length': [],
             'rewards': [],
             'completions': 0,
             'ppo_loss': [],
             'val_loss': []}
    
    value_fn = MLPValue(s_dims, val_fn_hidden_sizes)
    mse_loss = nn.MSELoss()
    
    policy_optimizer = torch.optim.Adam(policy.parameters(), lr = policy_lr)
    value_optimizer = torch.optim.Adam(value_fn.parameters(), lr = val_fn_lr)
    
    
    
    H = defaultdict(lambda: [])
    n_steps = 0
    for e in range(1, num_episodes + 1):
    
        if e % 1 == 0 or e == num_episodes:
            print(f'\rEpisode : {e}/{num_episodes} ({100*e/num_episodes:.2f}%)', end = '')
            
        T = 0
        s, _ = env.reset()
        stats['rewards'] += [0.0]
    
        while True:
            T += 1

            a, log_prob = policy.sample_action(torch.as_tensor(s))
            
            s_n, r, terminated, truncated, info = env.step(a.detach().numpy())
            
            n_steps += 1
            r = float(r)
            
            H['s'] += [s]
            H['a'] += [a]
            H['r'] += [r]
            H['s_n'] += [s_n]
            H['log_prob'] += [log_prob]
    
            stats['rewards'][-1] += r
    
            if n_steps >= steps_per_batch:
                states_tensor = torch.as_tensor(np.asarray(H['s']))
                next_states_tensor = torch.as_tensor(np.asarray(H['s_n']))
                actions_tensor = torch.as_tensor(np.asarray(H['a']))
                lprobs_tensor = torch.stack(H['log_prob']).detach()
    
                R = 0
                Rt_tensor = torch.zeros(n_steps)
                for t in reversed(range(n_steps)):
                    R = H['r'][t] + gamma * R
                    Rt_tensor[t] = R
    
                adv_tensor = Rt_tensor - value_fn(states_tensor).detach()
                adv_tensor = (adv_tensor - adv_tensor.mean()) / (adv_tensor.std() + 1e-8)
    
                dataset = torch.utils.data.TensorDataset(states_tensor,
                                                         actions_tensor,
                                                         Rt_tensor,
                                                         adv_tensor,
                                                         lprobs_tensor)
                
                minibatch_size = steps_per_batch//num_minibatches
                data_loader = torch.utils.data.DataLoader(dataset,
                                                          batch_size = minibatch_size,
                                                          shuffle = False)
    
                total_ppo_loss = 0.0
    
                for epoch in range(policy_epochs):
                    for states_mb, actions_mb, returns_mb, advantages_mb, old_lprobs_mb in data_loader:
                        if is_discrete:
                            probs = policy(states_mb)
                            dist = torch.distributions.Categorical(probs)
                            new_lprobs_mb = dist.log_prob(actions_mb)
                            dist_entropy = dist.entropy().mean()
                        else:
                            mu, std = policy(states_mb)
                            dist = torch.distributions.MultivariateNormal(mu, torch.diag_embed(std))
                            new_lprobs_mb = dist.log_prob(actions_mb)
                            dist_entropy = dist.entropy().mean()
                        
                        ratios = torch.exp(new_lprobs_mb - old_lprobs_mb)
                        clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)
                        ppo_loss = -torch.min(ratios * advantages_mb, clipped_ratios * advantages_mb).mean() - 0.01 * dist_entropy
                        total_ppo_loss += ppo_loss.item()
            
                        policy_optimizer.zero_grad()
                        ppo_loss.backward()
                        policy_optimizer.step()
    
                stats['ppo_loss'] += [total_ppo_loss/policy_epochs]
    
                val_avg_loss = 0.0
                for epoch in range(val_fn_epochs):
                    V_pred = value_fn(states_tensor).view(-1)
                    
                    value_optimizer.zero_grad()
                    
                    val_loss = mse_loss(V_pred, Rt_tensor)
                    val_avg_loss += val_loss.item()
                    
                    val_loss.backward()
                    value_optimizer.step()
                    
                val_avg_loss /= val_fn_epochs
                stats['val_loss'] += [val_avg_loss]
    
                H['s'] = []
                H['a'] = []
                H['r'] = []
                H['s_n'] = []
                H['log_prob'] = []
    
                n_steps = 0
            
            if terminated or truncated:
                ep_len_str = str(T)
                ep_len_str = "0"*(4-len(ep_len_str)) + ep_len_str
                print(f" -- ep_length = {ep_len_str} -- reward = {stats['rewards'][-1]:.2f}", end= '')
                
                stats['ep_length'] += [T]
                
                if truncated:
                    stats['completions'] += 1
                break
    
            s = s_n
        
        sys.stdout.flush()
    
    return stats


def PPO(env, policy, gamma = 1.0, num_episodes = 10,
        policy_lr = 1e-3, policy_epochs = 10, epsilon = 0.2,
        val_fn_lr = 1e-3, val_fn_epochs = 10, val_fn_hidden_sizes = [16],
        steps_per_batch = 1000, num_minibatches = 16):
        
    warnings.filterwarnings("ignore", category=DeprecationWarning)

    if isinstance(env.action_space, gym.spaces.Discrete):
         is_discrete = True
         s_dims = env.observation_space.shape[0]
         a_dims = env.action_space.n
    elif isinstance(env.action_space, gym.spaces.Box):
         is_discrete = False
         s_dims = env.observation_space.shape[0]
         a_dims = env.action_space.shape[0]
    else:
         raise NotImplementedError(f"Unsupported action space {type(env.action_space)}")
    
    stats = {'ep_length': [],
                    'rewards': [],
                    'completions': 0,
                    'ppo_loss': [],
                    'val_loss': []}
    
    value_fn = MLPValue(s_dims, val_fn_hidden_sizes)
    mse_loss = nn.MSELoss()

    policy_optimizer = torch.optim.Adam(policy.parameters(), lr = policy_lr)
    value_optimizer = torch.optim.Adam(value_fn.parameters(), lr = val_fn_lr)
    
    for e in range(1, num_episodes + 1):

        if e % 1 == 0 or e == num_episodes:
            print(f'\rEpisode : {e}/{num_episodes} ({100*e/num_episodes:.2f}%)', end = '')
            
    
        H = defaultdict(lambda: [])
        T = 0
        s, _ = env.reset()
        stats['rewards'] += [0.0]
        while True:
            T += 1
    
            if is_discrete: 
                probs = policy(torch.as_tensor(s))
                dist =  torch.distributions.Categorical(probs)
                a = dist.sample().item()
                a_np = a
                log_prob = dist.log_prob(torch.tensor(a))
            else:   
                mu, var = policy(torch.as_tensor(s))
                dist = torch.distributions.MultivariateNormal(mu, torch.diag(var))
                a = dist.sample()
                a_np = a.detach().numpy()
                log_prob = dist.log_prob(a)
                
                
            s_n, r, terminated, truncated, info = env.step(a_np)
    
            r = float(r)
            
            H['s'] += [s]
            H['a'] += [a]
            H['r'] += [r]
            H['log_prob'] += [log_prob]
    
            stats['rewards'][-1] += r
            if terminated or truncated:
                ep_len_str = str(T)
                ep_len_str = "0"*(4-len(ep_len_str)) + ep_len_str
                print(f" -- ep_length = {ep_len_str} -- reward = {stats['rewards'][-1]:.2f}", end= '')
                
                stats['ep_length'] += [T]
                
                if truncated:
                    stats['completions'] += 1
                break
    
            s = s_n
        
        R = 0
        Rt_tensor = torch.zeros(T)
        for t in reversed(range(T)):
            R = H['r'][t] + gamma * R
            Rt_tensor[t] = R
        
            
        states_tensor = torch.as_tensor(np.asarray(H['s']))
        actions_tensor = torch.as_tensor(np.asarray(H['a']))
        log_probs_tensor = torch.stack(H['log_prob'])

        val_avg_loss = 0.0
        for epoch in range(val_fn_epochs):
            V_pred = value_fn(states_tensor).view(-1)
            
            value_optimizer.zero_grad()
            
            val_loss = mse_loss(V_pred, Rt_tensor)
            val_avg_loss += val_loss.item()
            
            val_loss.backward()
            value_optimizer.step()
            
        val_avg_loss /= val_fn_epochs
        stats['val_loss'] += [val_avg_loss]
        
        adv_tensor = Rt_tensor - value_fn(states_tensor).detach()
        adv_tensor = (adv_tensor - adv_tensor.mean()) / (adv_tensor.std() + 1e-8)
    
        total_ppo_loss = 0.0
    
        old_probs_act = log_probs_tensor.detach()
        
        for epoch in range(policy_epochs):
            if is_discrete:
                probs = policy(states_tensor)
                dist = torch.distributions.Categorical(probs)
                new_probs_act = dist.log_prob(actions_tensor)
                dist_entropy = dist.entropy().mean()
            else:
                mu, var = policy(states_tensor)
                dist = torch.distributions.MultivariateNormal(mu, torch.diag_embed(var))
                new_probs_act = dist.log_prob(actions_tensor)
                dist_entropy = dist.entropy().mean()
            
            ratios = torch.exp(new_probs_act - old_probs_act)
            clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)
    
            ppo_loss = -torch.min(ratios * adv_tensor, clipped_ratios * adv_tensor).mean() - 0.001 * dist_entropy
            total_ppo_loss += ppo_loss.item()
            
            policy_optimizer.zero_grad()
            ppo_loss.backward()
            torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.5)
            policy_optimizer.step()
    
        stats['ppo_loss'] += [total_ppo_loss/policy_epochs]
        
        sys.stdout.flush()

    return stats


def PPO3(env, policy, gamma = 0.99,
         total_steps = 1e6, steps_per_batch = 2048,
         policy_lr = 1e-3, val_fn_lr = 1e-3, num_epochs = 10,
         epsilon = 0.2, val_fn_hidden_sizes = [16]):

    warnings.filterwarnings("ignore", category=DeprecationWarning)
    
    if isinstance(env.action_space, gym.spaces.Discrete):
        is_discrete = True
        s_dims = env.observation_space.shape[0]
        a_dims = env.action_space.n
    elif isinstance(env.action_space, gym.spaces.Box):
        is_discrete = False
        s_dims = env.observation_space.shape[0]
        a_dims = env.action_space.shape[0]
    else:
        raise NotImplementedError(f"Unsupported action space {type(env.action_space)}")
    
    stats = {'ep_length': [],
             'rewards': [],
             'completions': 0,
             'ppo_loss': [],
             'val_loss': [],
             'n_iters': 0}
    
    value_fn = MLPValue(s_dims, val_fn_hidden_sizes)
    mse_loss = nn.MSELoss()
    
    policy_optimizer = torch.optim.Adam(policy.parameters(), lr = policy_lr)
    value_optimizer = torch.optim.Adam(value_fn.parameters(), lr = val_fn_lr)
    
    
    n_steps = 0
    n_iters = 0
    e = 0
    print(f'\rStep : 0/{total_steps} (0.00%)', end = '')
    while n_steps <= total_steps:
        
        H = defaultdict(lambda: [])
        t=0
        num_eps = 0
        r_sum = 0.0
        while t <= steps_per_batch:
            H['episode_rewards'].append([])
            
            s, _ = env.reset()
            T = 0
            stats['rewards'] += [0.0]
            while True:
                if is_discrete: 
                    probs = policy(torch.as_tensor(s))
                    dist =  torch.distributions.Categorical(probs)
                    a = dist.sample().item()
                    a_np = a
                    log_prob = dist.log_prob(torch.tensor(a))
                else:   
                    mu, var = policy(torch.as_tensor(s))
                    dist = torch.distributions.MultivariateNormal(mu, torch.diag(var))
                    a = dist.sample()
                    a_np = a.detach().numpy()
                    log_prob = dist.log_prob(a)
                
                s_n, r, terminated, truncated, info = env.step(a_np)
                
                t += 1
                T += 1
                r = float(r)
                
                H['s'] += [s]
                H['a'] += [a]
                H['episode_rewards'][-1] += [r]
                H['s_n'] += [s_n]
                H['log_prob'] += [log_prob]
        
                stats['rewards'][-1] += r

                if terminated or truncated:
                    stats['ep_length'] += [T]
                    
                    if truncated:
                        stats['completions'] += 1

                    num_eps += 1
                    r_sum += stats['rewards'][-1]
                    break
                    
                s = s_n

        # The batch has been collected into H
        n_steps += t
        n_iters += 1
        stats['n_iters'] = n_iters
        avg_rsum = r_sum /num_eps
        
        states_tensor = torch.as_tensor(np.asarray(H['s']))
        next_states_tensor = torch.as_tensor(np.asarray(H['s_n']))
        actions_tensor = torch.as_tensor(np.asarray(H['a']))
        lprobs_tensor = torch.stack(H['log_prob']).detach()

        Rt_tensor = []
        for rewards in reversed(H['episode_rewards']):
            R = 0
            for r in reversed(rewards):
                R = r + gamma * R
                Rt_tensor.insert(0,R)
                
        Rt_tensor = torch.tensor(Rt_tensor)
        Rt_tensor = (Rt_tensor - Rt_tensor.mean()) / (Rt_tensor.std() + 1e-8)
        
        adv_tensor = Rt_tensor - value_fn(states_tensor).detach()
        adv_tensor = (adv_tensor - adv_tensor.mean()) / (adv_tensor.std() + 1e-8)
                    
        total_ppo_loss = 0.0
        total_val_loss = 0.0
        for epoch in range(num_epochs):
            
            V = value_fn(states_tensor).view(-1)
            
            val_loss = mse_loss(V, Rt_tensor)
            total_val_loss += val_loss.item()
            
            value_optimizer.zero_grad()
            val_loss.backward()
            value_optimizer.step()

            
            if is_discrete:
                probs = policy(states_tensor)
                dist = torch.distributions.Categorical(probs)
                new_lprobs = dist.log_prob(actions_tensor)
                dist_entropy = dist.entropy().mean()
            else:
                mu, var = policy(states_tensor)
                dist = torch.distributions.MultivariateNormal(mu, torch.diag_embed(var))
                new_lprobs = dist.log_prob(actions_tensor)
                dist_entropy = dist.entropy().mean()
                
            ratios = torch.exp(new_lprobs - lprobs_tensor)
            clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)
            ppo_loss = -torch.min(ratios * adv_tensor, clipped_ratios * adv_tensor).mean() - 0.0 * dist_entropy
            total_ppo_loss += ppo_loss.item()

            policy_optimizer.zero_grad()
            ppo_loss.backward()
            policy_optimizer.step()

            
        stats['val_loss'] += [total_val_loss/num_epochs]
        stats['ppo_loss'] += [total_ppo_loss/num_epochs]
        
        sys.stdout.flush()

        if n_steps % 1 == 0 or n_steps >= total_steps:
            print(f'\rStep : {n_steps}/{total_steps} ({100*n_steps/total_steps:.2f}%) -- Reward : {avg_rsum:.2f}', end = '')
    
    return stats





s_dims = cartpole_env.observation_space.shape[0]
a_dims = cartpole_env.action_space.n

cartpole_policy = MLPPolicyDiscrete(s_dims, a_dims, CONFIG['MLPPolicy_hidden_sizes'])
cartpole_stats = PPO3(cartpole_env, cartpole_policy, **CONFIG['PPO']['CartPole-v1'])


animate_policy(cartpole_env, cartpole_policy, **CONFIG['animation'])


plot_stats(cartpole_stats, CONFIG['plot_stats_size'])


s_dims = lunar_env.observation_space.shape[0]
a_dims = lunar_env.action_space.n

lunar_policy = MLPPolicyDiscrete(s_dims, a_dims, CONFIG['MLPPolicy_hidden_sizes'])
lunar_stats = PPO3(lunar_env, lunar_policy, **CONFIG['PPO']['LunarLander-v3'])


animate_policy(lunar_env, lunar_policy, **CONFIG['animation'])


plot_stats(lunar_stats, CONFIG['plot_stats_size'])


s_dims = acrobot_env.observation_space.shape[0]
a_dims = acrobot_env.action_space.n

acrobot_policy = MLPPolicyDiscrete(s_dims, a_dims, CONFIG['MLPPolicy_hidden_sizes'])
acrobot_stats = PPO(acrobot_env, acrobot_policy, **CONFIG['PPO']['Acrobot-v1'])


animate_policy(acrobot_env, acrobot_policy, **CONFIG['animation'])


plot_stats(acrobot_stats, CONFIG['plot_stats_size'])





s_dims = pendulum_env.observation_space.shape[0]
a_dims = pendulum_env.action_space.shape[0]

pendulum_policy = MLPPolicyContinuous2(s_dims, a_dims, CONFIG['MLPPolicy_hidden_sizes'])
pendulum_stats = PPO3(pendulum_env, pendulum_policy, **CONFIG['PPO']['Pendulum-v1'])


animate_policy(pendulum_env, pendulum_policy, **CONFIG['animation'])


plot_stats(pendulum_stats, 1000)


s_dims = bipedal_env.observation_space.shape[0]
a_dims = bipedal_env.action_space.shape[0]

bipedal_policy = MLPPolicyContinuous(s_dims, a_dims, CONFIG['MLPPolicy_hidden_sizes'])
bipedal_stats = PPO(bipedal_env, bipedal_policy, **CONFIG['PPO']['BipedalWalker-v3'])


animate_policy(bipedal_env, bipedal_policy, **CONFIG['animation'])


plot_stats(bipedal_stats, CONFIG['plot_stats_size'])



