# RLFromScratch

This repo contains a set of notebooks that covers from-scratch implementations of RL algorithms from Tabular RL to Deep RL.
The implementations themselves are very basic and simplistic and are not supposed to be used for actual RL work.
They are supposed to be learning tools to understand how one could naively implement the major RL algorithms in code.

The Tabular RL algorithms are implemented with [David Silver's RL course](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) as the guideline and the Deep RL aglorithms are implemented with [OpenAI Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/) as the guideline. I recommend watching/reading through both of these if you are starting out with Reinforcement Learning and use these notebooks as a reference.

## Algorithms implemented

### **Tabular RL**
  -   Policy Iteration
  -   Value Iteration
  -   MonteCarlo Prediction and Control
  -   SARSALambda
  -   Q-Learning
    
### **Deep RL**
  -   Vanilla Policy Gradient (VPG)
  -   Deep Q Network (DQN)
  -   Trust Region Policy Optimization (TRPO)
  -   Proximal Policy Optimization (PPO)
  -   Deep Deterministic Policy Gradient (DDPG)
  -   Twin Delayed Deep Deterministic Policy Gradient (TD3)


